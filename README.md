# Pretrained-Transformer-Text-Generation

In this experiment I used the GPT 2 Transformer model from hugging face to run a series of text generation decoding experiments.
I compared a variety of decoding strategies which consisted of greedy search, beam search and sampling methods with varying tempuratures, topK, and topP values. This experiment demonstrates the different use cases of each decoding strategy by displaying the strength and weaknesses of each model through the generated text.
